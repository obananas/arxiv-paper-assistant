import requests
import datetime
import smtplib
import email.utils
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
from openai import OpenAI
import xml.etree.ElementTree as ET
import os
import csv
from urllib.parse import quote_plus
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

def get_yesterday_utc():
    """è·å–UTCæ—¶é—´å‰ä¸€å¤©"""
    return (datetime.datetime.now(datetime.timezone.utc) 
            - datetime.timedelta(days=1)).strftime('%Y-%m-%d')

def safe_xml_text(element, path, namespaces, default="N/A"):
    """å®‰å…¨è·å–XMLæ–‡æœ¬"""
    elem = element.find(path, namespaces)
    return elem.text.strip() if elem is not None and elem.text else default

def fetch_arxiv_papers(search_term, target_date, max_retries=3):
    """å¸¦é‡è¯•å’Œåˆ†é¡µçš„è®ºæ–‡è·å–"""
    papers = []
    start = 0
    max_results = 100  # æ¯é¡µæœ€å¤§æ•°é‡
    base_url = "http://export.arxiv.org/api/query?"
    
    while True:
        for attempt in range(max_retries):
            try:
                # æ„é€ è¯·æ±‚å‚æ•°
                params = {
                    "search_query": f'(ti:{quote_plus(search_term)} OR abs:{quote_plus(search_term)}) AND cat:cs.*',
                    "start": start,
                    "max_results": max_results,
                    "sortBy": "submittedDate",
                    "sortOrder": "descending"
                }
                response = requests.get(base_url, params=params, timeout=45)
                response.raise_for_status()
                
                root = ET.fromstring(response.content)
                entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')
                if not entries:
                    return papers  # æ— æ›´å¤šæ•°æ®

                # è§£ææ¡ç›®
                new_papers = []
                for entry in entries:
                    pub_date = datetime.datetime.strptime(
                        safe_xml_text(entry, './atom:published', namespaces),
                        "%Y-%m-%dT%H:%M:%SZ"
                    ).strftime("%Y-%m-%d")
                    
                    if pub_date != target_date:
                        continue
                        
                    paper = {
                        "arxiv_id": entry.find('./atom:id', namespaces).text.split('/')[-1],
                        "title": safe_xml_text(entry, './atom:title', namespaces),
                        "summary": safe_xml_text(entry, './atom:summary', namespaces),
                        "pub_date": pub_date,
                        "authors": [safe_xml_text(a, './atom:name', namespaces) 
                                  for a in entry.findall('./atom:author', namespaces)],
                        "categories": [c.get('term') 
                                     for c in entry.findall('./atom:category', namespaces)],
                        "comments": safe_xml_text(entry, './arxiv:comment', namespaces),
                    }
                    new_papers.append(paper)
                
                papers.extend(new_papers)
                start += len(entries)
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

            except (requests.RequestException, ET.ParseError) as e:
                logging.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
                time.sleep(5 * (attempt + 1))
                continue
        else:
            logging.error(f"æ— æ³•è·å– {search_term} çš„è®ºæ–‡")
            break
            
    return papers

def robust_ai_process(text, prompt_template, config, max_length=6000):
    """å¸¦é•¿åº¦æ£€æŸ¥å’Œé‡è¯•çš„AIå¤„ç†"""
    text = text[:max_length]  # é˜²æ­¢è¶…é•¿æ–‡æœ¬
    
    for _ in range(3):
        try:
            result = process_with_openai(
                text, prompt_template,
                config['openai']['api_key'],
                config['openai']['model'],
                config['openai']['api_base']
            )
            if result and len(result) > 20:  # éªŒè¯æœ‰æ•ˆå“åº”
                return result
        except Exception as e:
            logging.warning(f"AIå¤„ç†å¤±è´¥: {str(e)}")
            time.sleep(2)
    
    return "âš ï¸ å†…å®¹ç”Ÿæˆå¤±è´¥"

def format_paper(paper):
    """å®¹é”™æ€§æ›´å¼ºçš„æ ¼å¼åŒ–"""
    parts = [
        f"ğŸ“„ æ ‡é¢˜: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}",
        f"ğŸ‘¥ ä½œè€…: {', '.join(paper.get('authors', ['æœªçŸ¥ä½œè€…']))}",
        f"ğŸ·ï¸ åˆ†ç±»: {', '.join(paper.get('categories', ['æœªåˆ†ç±»']))}",
        f"ğŸ“… æ—¥æœŸ: {paper.get('pub_date', 'æœªçŸ¥æ—¥æœŸ')}",
    ]
    
    if paper.get('comments'):
        parts.append(f"ğŸ’¬ è¯„è®º: {paper['comments']}")
        
    parts += [
        f"ğŸ”— é“¾æ¥: https://arxiv.org/abs/{paper['arxiv_id']}",
        f"ğŸ“„ PDF: https://arxiv.org/pdf/{paper['arxiv_id']}.pdf",
    ]
    
    if 'contribution' in paper:
        parts.append(f"\nğŸŒŸ è´¡çŒ®è¦ç‚¹:\n{paper['contribution']}")
    
    if 'summary_zh' in paper:
        parts.append(f"\nğŸŒ ä¸­æ–‡æ‘˜è¦:\n{paper['summary_zh']}")
    
    parts.append(f"\nğŸ“œ åŸå§‹æ‘˜è¦:\n{paper.get('summary', 'æ— æ‘˜è¦')}")
    
    return "\n".join(parts) + "\n" + "-"*70 + "\n"

if __name__ == "__main__":
    # é…ç½®åˆå§‹åŒ–
    config = {
        "sender": {
            "email": os.getenv("SENDER_EMAIL"),
            "password": os.getenv("SENDER_PASSWORD"),
            "smtp_server": "smtp.qq.com",
            "smtp_port": 465
        },
        "openai": {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "model": os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            "api_base": os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        }
    }
    
    # å…³é”®è¯è§£æ
    search_terms = next(csv.reader([os.getenv("SEARCH_TERMS", "")])) if os.getenv("SEARCH_TERMS") else []
    
    # ä¸»æµç¨‹
    target_date = get_yesterday_utc()
    all_papers = {}
    
    for term in search_terms:
        logging.info(f"å¤„ç†å…³é”®è¯: {term}")
        papers = fetch_arxiv_papers(term, target_date)
        
        for paper in papers:
            pid = paper["arxiv_id"]
            if pid not in all_papers:
                # å¹¶å‘å¤„ç†AIè¯·æ±‚
                paper["summary_zh"] = robust_ai_process(
                    paper["summary"], 
                    "ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œä¿ç•™æœ¯è¯­:\n{text}",
                    config
                )
                paper["contribution"] = robust_ai_process(
                    paper["summary"],
                    "ç”¨ä¸­æ–‡æ€»ç»“æ ¸å¿ƒè´¡çŒ®:\n{text}",
                    config
                )
                all_papers[pid] = paper
                logging.info(f"å·²å¤„ç†è®ºæ–‡: {pid[:10]}...")

    # å‘é€é‚®ä»¶
    if all_papers:
        content = f"arXivè®ºæ–‡æ—¥æŠ¥ {target_date}\n\n" + "\n".join([format_paper(p) for p in all_papers.values()])
        send_email(
            subject=f"arXivæ—¥æŠ¥ {target_date} - {len(all_papers)}ç¯‡",
            content=content,
            sender_info=config["sender"],
            receiver_emails=os.getenv("RECEIVER_EMAILS", "").split(",")
        )
